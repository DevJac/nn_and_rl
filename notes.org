* TODO Note various accuracy measurements, like F1 score
* TODO Note training methods
** Cross-entropy
Train on the best episodes and discard the rest.
** Bellman equations
These are the basis of reinforcement learning and dynamic programming optimizations.
** V(s) vs Q(s, a) and Q-learning
V(s) doesn't help us determine what action to take.
Q(s, a) does help us determine what action to take.
** Value iteration
The value of a state is the reward it can receive immediately, plus the value of the next state. This is recursive.

You can recursively update the value function. This "spreads the value around" after many iterations.

I suspect this may provided added stability to a neural networks based agent as well. I will try it.
* Idea: Learn State Transitions
Can we train a model to predict state transitions? That could serve as a model of the environment. The value / policy could then be updated using the simulated environment rather than the real environment.

This would probably have little value in virtual environment, like OpenAI Gym, because the environments are already simulated, so you mine as well use the perfectly simulated environment you already have.
